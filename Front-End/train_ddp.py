# train_ddp.py (å·²åŠ å…¥DDPåŒæ­¥é€»è¾‘ï¼Œå¯å½»åº•è§£å†³ç«äº‰é—®é¢˜)
import os
import argparse
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch_geometric.loader import DataLoader
from tqdm import tqdm

from li_dataset import LiClusterDataset
from model import SchNetModel

def setup_ddp(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup_ddp():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    dist.destroy_process_group()

def main(rank, world_size, args):
    # å‡å°‘è°ƒè¯•ä¿¡æ¯è¾“å‡º
    if rank == 0:
        print(f"åœ¨ Rank {rank} ä¸Šè¿è¡Œ DDP è®­ç»ƒï¼Œæ€»è¿›ç¨‹æ•°: {world_size}ã€‚")
    setup_ddp(rank, world_size)

    # ######################################################################
    # ## è¿™æ˜¯è§£å†³é—®é¢˜çš„å…³é”®ï¼šç¡®ä¿åªæœ‰ Rank 0 å¤„ç†æ•°æ®ï¼Œå…¶ä»–è¿›ç¨‹ç­‰å¾… ##
    # ######################################################################
    if rank == 0:
        # ä¸»è¿›ç¨‹è´Ÿè´£æ£€æŸ¥æ•°æ®å’Œé¢„å¤„ç†ã€‚
        # å¦‚æœ li_dataset_processed æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè¿™è¡Œä»£ç ä¼šè§¦å‘ .process()
        print("æ­£åœ¨åˆå§‹åŒ–æ•°æ®é›†ï¼Œå¦‚æœéœ€è¦ï¼Œå°†æ‰§è¡Œé¢„å¤„ç†...")
        LiClusterDataset(root=args.data_root, data_path=args.data_file)
        print("æ•°æ®é›†æ£€æŸ¥/å¤„ç†å®Œæˆã€‚")

    # è®¾ç½®ä¸€ä¸ªå±éšœï¼Œæ‰€æœ‰è¿›ç¨‹å¿…é¡»åœ¨è¿™é‡Œç­‰å¾…ï¼Œç›´åˆ° rank 0 å®Œæˆä¸Šé¢çš„ä»»åŠ¡
    dist.barrier()
    
    # å±éšœè¢«æ‰“ç ´åï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½å¯ä»¥å®‰å…¨åœ°åˆå§‹åŒ–æ•°æ®é›†äº†ï¼Œ
    # å› ä¸ºå®ƒä»¬ç°åœ¨å¯ä»¥ç¡®å®šé¢„å¤„ç†å·²ç»å®Œæˆï¼Œå¯ä»¥ç›´æ¥ä»ç£ç›˜åŠ è½½ã€‚
    dataset = LiClusterDataset(root=args.data_root, data_path=args.data_file)
    if rank == 0:
        print("æ•°æ®é›†åŠ è½½æˆåŠŸã€‚")
    # ######################################################################
    # ## åŒæ­¥é€»è¾‘ç»“æŸ ##
    # ######################################################################

    # --- æ•°æ®å‡†å¤‡ ---
    # ä½¿ç”¨æ ‡å‡†çš„ 70%/15%/15% åˆ’åˆ†
    total_size = len(dataset)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    test_size = total_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size, test_size]
    )
    
    if rank == 0:
        print(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†={train_size} ({train_size/total_size*100:.1f}%), "
              f"éªŒè¯é›†={val_size} ({val_size/total_size*100:.1f}%), "
              f"æµ‹è¯•é›†={test_size} ({test_size/total_size*100:.1f}%)")
    
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler, num_workers=4)
    if rank == 0:
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # --- æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    model = SchNetModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank], find_unused_parameters=False) # find_unused_parameters è®¾ä¸ºFalseå¯æé€Ÿ
    loss_fn = torch.nn.L1Loss()
    optimizer = torch.optim.Adam(ddp_model.parameters(), lr=args.lr)
    
    best_val_mae = float('inf')

    # --- è®­ç»ƒå¾ªç¯ ---
    for epoch in range(args.epochs):
        train_sampler.set_epoch(epoch)
        ddp_model.train()
        
        pbar = None
        if rank == 0:
            pbar = tqdm(total=len(train_loader), desc=f"Epoch {epoch+1}/{args.epochs} [Train]")
        
        for batch in train_loader:
            batch = batch.to(rank)
            optimizer.zero_grad()
            pred = ddp_model(batch)
            loss = loss_fn(pred.squeeze(), batch.y)
            loss.backward()
            optimizer.step()
            
            if rank == 0:
                pbar.update(1)
        
        if rank == 0:
            pbar.close()

        # --- éªŒè¯ä¸æ¨¡å‹ä¿å­˜ (åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œ) ---
        if rank == 0:
            ddp_model.eval()
            total_val_mae = 0
            with torch.no_grad():
                for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{args.epochs} [Val]"):
                    batch = batch.to(rank)
                    pred_normalized = ddp_model.module(batch)
                    pred_real = pred_normalized * dataset.std + dataset.mean
                    true_real = batch.y * dataset.std + dataset.mean
                    total_val_mae += torch.nn.functional.l1_loss(pred_real.squeeze(), true_real).item()
            
            avg_val_mae = total_val_mae / len(val_loader)
            print(f"Epoch {epoch+1}, Avg Val MAE (Real): {avg_val_mae:.6f} eV")

            if avg_val_mae < best_val_mae:
                best_val_mae = avg_val_mae
                torch.save(ddp_model.module.state_dict(), 'best_schnet_model.pt')
                print(f"*** æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯ MAE: {best_val_mae:.6f} eV ***")

    # --- è®­ç»ƒç»“æŸååœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹ ---
    if rank == 0:
        print("\n" + "="*60)
        print("è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        ddp_model.module.load_state_dict(torch.load('best_schnet_model.pt'))
        ddp_model.eval()
        
        total_test_mae = 0
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                batch = batch.to(rank)
                pred_normalized = ddp_model.module(batch)
                pred_real = pred_normalized * dataset.std + dataset.mean
                true_real = batch.y * dataset.std + dataset.mean
                total_test_mae += torch.nn.functional.l1_loss(pred_real.squeeze(), true_real).item()
        
        avg_test_mae = total_test_mae / len(test_loader)
        print(f"\nğŸ¯ æœ€ç»ˆæµ‹è¯•é›† MAE: {avg_test_mae:.6f} eV")
        print(f"ğŸ“Š æœ€ä½³éªŒè¯é›† MAE: {best_val_mae:.6f} eV")
        print(f"ğŸ“ˆ æ³›åŒ–æ€§èƒ½ (æµ‹è¯•-éªŒè¯): {avg_test_mae - best_val_mae:+.6f} eV")
        print("="*60)

    cleanup_ddp()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Distributed SchNet Training")
    parser.add_argument('--data_root', type=str, default='./li_dataset_processed', help='æ–‡ä»¶å¤¹ï¼Œç”¨äºå­˜æ”¾å¤„ç†å¥½çš„æ•°æ®')
    parser.add_argument('--data_file', type=str, default='/data/Hackthon/data/TheDataOfClusters_4_40 copy.data', help='åŸå§‹æ•°æ®æ–‡ä»¶çš„è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=200, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=128, help='æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    args = parser.parse_args()

    world_size = torch.cuda.device_count()
    if world_size > 0:
        print(f"æ£€æµ‹åˆ° {world_size} å¼  GPUï¼Œå¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒã€‚")
        torch.multiprocessing.spawn(main, args=(world_size, args), nprocs=world_size, join=True)
    else:
        print("æœªæ£€æµ‹åˆ°GPUï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®ã€‚")